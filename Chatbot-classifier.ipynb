{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "40c22861-3922-4208-84c1-5475d0c6c956",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch\n",
      "  Using cached torch-2.5.1-cp311-cp311-manylinux1_x86_64.whl.metadata (28 kB)\n",
      "Collecting filelock (from torch)\n",
      "  Using cached filelock-3.16.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.11/site-packages (from torch) (4.8.0)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.11/site-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.11/site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.11/site-packages (from torch) (2023.12.0)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
      "  Using cached nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
      "  Using cached nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
      "  Using cached nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
      "  Using cached nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
      "  Using cached nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
      "  Using cached nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
      "  Using cached nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
      "  Using cached nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
      "  Using cached nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-nccl-cu12==2.21.5 (from torch)\n",
      "  Using cached nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-nvtx-cu12==12.4.127 (from torch)\n",
      "  Using cached nvidia_nvtx_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
      "  Using cached nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting triton==3.1.0 (from torch)\n",
      "  Using cached triton-3.1.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.3 kB)\n",
      "Collecting sympy==1.13.1 (from torch)\n",
      "  Using cached sympy-1.13.1-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.11/site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.11/site-packages (from jinja2->torch) (2.1.3)\n",
      "Using cached torch-2.5.1-cp311-cp311-manylinux1_x86_64.whl (906.5 MB)\n",
      "Using cached nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
      "Using cached nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
      "Using cached nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
      "Using cached nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
      "Using cached nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
      "Using cached nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
      "Using cached nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
      "Using cached nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
      "Using cached nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
      "Using cached nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl (188.7 MB)\n",
      "Using cached nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
      "Using cached nvidia_nvtx_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (99 kB)\n",
      "Using cached sympy-1.13.1-py3-none-any.whl (6.2 MB)\n",
      "Using cached triton-3.1.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (209.5 MB)\n",
      "Using cached filelock-3.16.1-py3-none-any.whl (16 kB)\n",
      "Installing collected packages: sympy, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, filelock, triton, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torch\n",
      "  Attempting uninstall: sympy\n",
      "    Found existing installation: sympy 1.12\n",
      "    Uninstalling sympy-1.12:\n",
      "      Successfully uninstalled sympy-1.12\n",
      "Successfully installed filelock-3.16.1 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nccl-cu12-2.21.5 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.4.127 sympy-1.13.1 torch-2.5.1 triton-3.1.0\n"
     ]
    }
   ],
   "source": [
    "#!pip install beautifulsoup4\n",
    "#!pip install --upgrade nltk\n",
    "!pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "14ee8f03-d1f7-4ab6-af5d-16f5c0e067f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on GPU!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import string\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from nltk.stem import PorterStemmer\n",
    "from bs4 import BeautifulSoup\n",
    "from html import unescape\n",
    "\n",
    "np.random.seed(7)\n",
    "\n",
    "train_on_gpu = torch.cuda.is_available()\n",
    "if train_on_gpu:\n",
    "    print('Training on GPU!')\n",
    "else:\n",
    "    print('No GPU available, training on CPU; consider making n_epochs very small.')\n",
    "\n",
    "questions_df = pd.read_csv('dataset/Questions.csv', nrows=5000, encoding='ISO-8859-1')\n",
    "answers_df = pd.read_csv('dataset/Answers.csv', nrows=5000, encoding='ISO-8859-1')\n",
    "tags_df = pd.read_csv('dataset/Tags.csv', nrows=5000, encoding='ISO-8859-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ffa87f3a-d941-437a-8be7-ba76ec6ac315",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>OwnerUserId</th>\n",
       "      <th>CreationDate</th>\n",
       "      <th>Score</th>\n",
       "      <th>Title</th>\n",
       "      <th>Body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>469</td>\n",
       "      <td>147.0</td>\n",
       "      <td>2008-08-02T15:11:16Z</td>\n",
       "      <td>21</td>\n",
       "      <td>How can I find the full path to a font from it...</td>\n",
       "      <td>&lt;p&gt;I am using the Photoshop's javascript API t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>502</td>\n",
       "      <td>147.0</td>\n",
       "      <td>2008-08-02T17:01:58Z</td>\n",
       "      <td>27</td>\n",
       "      <td>Get a preview JPEG of a PDF on Windows?</td>\n",
       "      <td>&lt;p&gt;I have a cross-platform (Python) applicatio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>535</td>\n",
       "      <td>154.0</td>\n",
       "      <td>2008-08-02T18:43:54Z</td>\n",
       "      <td>40</td>\n",
       "      <td>Continuous Integration System for a Python Cod...</td>\n",
       "      <td>&lt;p&gt;I'm starting work on a hobby project with a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>594</td>\n",
       "      <td>116.0</td>\n",
       "      <td>2008-08-03T01:15:08Z</td>\n",
       "      <td>25</td>\n",
       "      <td>cx_Oracle: How do I iterate over a result set?</td>\n",
       "      <td>&lt;p&gt;There are several ways to iterate over a re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>683</td>\n",
       "      <td>199.0</td>\n",
       "      <td>2008-08-03T13:19:16Z</td>\n",
       "      <td>28</td>\n",
       "      <td>Using 'in' to match an attribute of Python obj...</td>\n",
       "      <td>&lt;p&gt;I don't remember whether I was dreaming or ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>742</td>\n",
       "      <td>189.0</td>\n",
       "      <td>2008-08-03T15:55:28Z</td>\n",
       "      <td>30</td>\n",
       "      <td>Class views in Django</td>\n",
       "      <td>&lt;p&gt;&lt;a href=\"http://www.djangoproject.com/\"&gt;Dja...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>766</td>\n",
       "      <td>1384652.0</td>\n",
       "      <td>2008-08-03T17:44:07Z</td>\n",
       "      <td>20</td>\n",
       "      <td>Python and MySQL</td>\n",
       "      <td>&lt;p&gt;I can get Python to work with Postgresql bu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>773</td>\n",
       "      <td>207.0</td>\n",
       "      <td>2008-08-03T18:27:09Z</td>\n",
       "      <td>256</td>\n",
       "      <td>How do I use Python's itertools.groupby()?</td>\n",
       "      <td>&lt;p&gt;I haven't been able to find an understandab...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>972</td>\n",
       "      <td>145.0</td>\n",
       "      <td>2008-08-04T02:17:51Z</td>\n",
       "      <td>364</td>\n",
       "      <td>Adding a Method to an Existing Object Instance</td>\n",
       "      <td>&lt;p&gt;I've read that it is possible to add a meth...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1476</td>\n",
       "      <td>92.0</td>\n",
       "      <td>2008-08-04T18:20:36Z</td>\n",
       "      <td>251</td>\n",
       "      <td>How do you express binary literals in Python?</td>\n",
       "      <td>&lt;p&gt;How do you express an integer as a binary n...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Id  OwnerUserId          CreationDate  Score  \\\n",
       "0   469        147.0  2008-08-02T15:11:16Z     21   \n",
       "1   502        147.0  2008-08-02T17:01:58Z     27   \n",
       "2   535        154.0  2008-08-02T18:43:54Z     40   \n",
       "3   594        116.0  2008-08-03T01:15:08Z     25   \n",
       "4   683        199.0  2008-08-03T13:19:16Z     28   \n",
       "5   742        189.0  2008-08-03T15:55:28Z     30   \n",
       "6   766    1384652.0  2008-08-03T17:44:07Z     20   \n",
       "7   773        207.0  2008-08-03T18:27:09Z    256   \n",
       "8   972        145.0  2008-08-04T02:17:51Z    364   \n",
       "9  1476         92.0  2008-08-04T18:20:36Z    251   \n",
       "\n",
       "                                               Title  \\\n",
       "0  How can I find the full path to a font from it...   \n",
       "1            Get a preview JPEG of a PDF on Windows?   \n",
       "2  Continuous Integration System for a Python Cod...   \n",
       "3     cx_Oracle: How do I iterate over a result set?   \n",
       "4  Using 'in' to match an attribute of Python obj...   \n",
       "5                              Class views in Django   \n",
       "6                                   Python and MySQL   \n",
       "7         How do I use Python's itertools.groupby()?   \n",
       "8     Adding a Method to an Existing Object Instance   \n",
       "9      How do you express binary literals in Python?   \n",
       "\n",
       "                                                Body  \n",
       "0  <p>I am using the Photoshop's javascript API t...  \n",
       "1  <p>I have a cross-platform (Python) applicatio...  \n",
       "2  <p>I'm starting work on a hobby project with a...  \n",
       "3  <p>There are several ways to iterate over a re...  \n",
       "4  <p>I don't remember whether I was dreaming or ...  \n",
       "5  <p><a href=\"http://www.djangoproject.com/\">Dja...  \n",
       "6  <p>I can get Python to work with Postgresql bu...  \n",
       "7  <p>I haven't been able to find an understandab...  \n",
       "8  <p>I've read that it is possible to add a meth...  \n",
       "9  <p>How do you express an integer as a binary n...  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "questions_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dcf82eb5-8c00-4f8e-a421-3a8f2e3cef57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>OwnerUserId</th>\n",
       "      <th>CreationDate</th>\n",
       "      <th>ParentId</th>\n",
       "      <th>Score</th>\n",
       "      <th>Body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>497</td>\n",
       "      <td>50.0</td>\n",
       "      <td>2008-08-02T16:56:53Z</td>\n",
       "      <td>469</td>\n",
       "      <td>4</td>\n",
       "      <td>&lt;p&gt;open up a terminal (Applications-&amp;gt;Utilit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>518</td>\n",
       "      <td>153.0</td>\n",
       "      <td>2008-08-02T17:42:28Z</td>\n",
       "      <td>469</td>\n",
       "      <td>2</td>\n",
       "      <td>&lt;p&gt;I haven't been able to find anything that d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>536</td>\n",
       "      <td>161.0</td>\n",
       "      <td>2008-08-02T18:49:07Z</td>\n",
       "      <td>502</td>\n",
       "      <td>9</td>\n",
       "      <td>&lt;p&gt;You can use ImageMagick's convert utility f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>538</td>\n",
       "      <td>156.0</td>\n",
       "      <td>2008-08-02T18:56:56Z</td>\n",
       "      <td>535</td>\n",
       "      <td>23</td>\n",
       "      <td>&lt;p&gt;One possibility is Hudson.  It's written in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>541</td>\n",
       "      <td>157.0</td>\n",
       "      <td>2008-08-02T19:06:40Z</td>\n",
       "      <td>535</td>\n",
       "      <td>20</td>\n",
       "      <td>&lt;p&gt;We run &lt;a href=\"http://buildbot.net/trac\"&gt;B...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>595</td>\n",
       "      <td>116.0</td>\n",
       "      <td>2008-08-03T01:17:36Z</td>\n",
       "      <td>594</td>\n",
       "      <td>25</td>\n",
       "      <td>&lt;p&gt;The canonical way is to use the built-in cu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>660</td>\n",
       "      <td>197.0</td>\n",
       "      <td>2008-08-03T12:09:18Z</td>\n",
       "      <td>535</td>\n",
       "      <td>14</td>\n",
       "      <td>&lt;p&gt;Second the Buildbot - Trac integration. You...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>701</td>\n",
       "      <td>111.0</td>\n",
       "      <td>2008-08-03T14:30:50Z</td>\n",
       "      <td>683</td>\n",
       "      <td>3</td>\n",
       "      <td>&lt;p&gt;No, you were not dreaming.  Python has a pr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>735</td>\n",
       "      <td>145.0</td>\n",
       "      <td>2008-08-03T15:47:22Z</td>\n",
       "      <td>683</td>\n",
       "      <td>-2</td>\n",
       "      <td>&lt;p&gt;I think:&lt;/p&gt;\\r\\n\\r\\n&lt;pre&gt;&lt;code&gt;#!/bin/pytho...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>745</td>\n",
       "      <td>154.0</td>\n",
       "      <td>2008-08-03T15:59:19Z</td>\n",
       "      <td>683</td>\n",
       "      <td>8</td>\n",
       "      <td>&lt;p&gt;Are you looking to get a list of objects th...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Id  OwnerUserId          CreationDate  ParentId  Score  \\\n",
       "0  497         50.0  2008-08-02T16:56:53Z       469      4   \n",
       "1  518        153.0  2008-08-02T17:42:28Z       469      2   \n",
       "2  536        161.0  2008-08-02T18:49:07Z       502      9   \n",
       "3  538        156.0  2008-08-02T18:56:56Z       535     23   \n",
       "4  541        157.0  2008-08-02T19:06:40Z       535     20   \n",
       "5  595        116.0  2008-08-03T01:17:36Z       594     25   \n",
       "6  660        197.0  2008-08-03T12:09:18Z       535     14   \n",
       "7  701        111.0  2008-08-03T14:30:50Z       683      3   \n",
       "8  735        145.0  2008-08-03T15:47:22Z       683     -2   \n",
       "9  745        154.0  2008-08-03T15:59:19Z       683      8   \n",
       "\n",
       "                                                Body  \n",
       "0  <p>open up a terminal (Applications-&gt;Utilit...  \n",
       "1  <p>I haven't been able to find anything that d...  \n",
       "2  <p>You can use ImageMagick's convert utility f...  \n",
       "3  <p>One possibility is Hudson.  It's written in...  \n",
       "4  <p>We run <a href=\"http://buildbot.net/trac\">B...  \n",
       "5  <p>The canonical way is to use the built-in cu...  \n",
       "6  <p>Second the Buildbot - Trac integration. You...  \n",
       "7  <p>No, you were not dreaming.  Python has a pr...  \n",
       "8  <p>I think:</p>\\r\\n\\r\\n<pre><code>#!/bin/pytho...  \n",
       "9  <p>Are you looking to get a list of objects th...  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answers_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "07cf187c-dfb7-4ef3-8f56-4ceba2f9c693",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>469</td>\n",
       "      <td>python</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>469</td>\n",
       "      <td>osx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>469</td>\n",
       "      <td>fonts</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>469</td>\n",
       "      <td>photoshop</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>502</td>\n",
       "      <td>python</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>502</td>\n",
       "      <td>windows</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>502</td>\n",
       "      <td>image</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>502</td>\n",
       "      <td>pdf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>535</td>\n",
       "      <td>python</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>535</td>\n",
       "      <td>continuous-integration</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Id                     Tag\n",
       "0  469                  python\n",
       "1  469                     osx\n",
       "2  469                   fonts\n",
       "3  469               photoshop\n",
       "4  502                  python\n",
       "5  502                 windows\n",
       "6  502                   image\n",
       "7  502                     pdf\n",
       "8  535                  python\n",
       "9  535  continuous-integration"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tags_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cd3103c2-a0fa-41c5-a80f-db9cb89eba98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Net(\n",
      "  (embedding): Embedding(46371, 20)\n",
      "  (lstm): LSTM(20, 16, batch_first=True)\n",
      "  (fc1): Linear(in_features=16, out_features=256, bias=True)\n",
      "  (fc2): Linear(in_features=256, out_features=1154, bias=True)\n",
      ")\n",
      "Parameters [927420, 1280, 1024, 64, 64, 4096, 256, 295424, 1154]\n",
      "Step 0, Loss: 0.2501976490020752\n",
      "Step 25, Loss: 0.2501925528049469\n",
      "Step 50, Loss: 0.2501874566078186\n",
      "Step 75, Loss: 0.2501823902130127\n",
      "Step 100, Loss: 0.250177264213562\n",
      "Step 125, Loss: 0.2501721978187561\n",
      "Step 150, Loss: 0.2501671314239502\n",
      "Step 175, Loss: 0.2501620352268219\n",
      "Step 200, Loss: 0.250156968832016\n",
      "Step 225, Loss: 0.2501518726348877\n",
      "Step 250, Loss: 0.2501468062400818\n",
      "Step 275, Loss: 0.2501417100429535\n",
      "Step 300, Loss: 0.2501366436481476\n",
      "Step 325, Loss: 0.2501315772533417\n",
      "Step 350, Loss: 0.2501264810562134\n",
      "Step 375, Loss: 0.25012141466140747\n",
      "Step 400, Loss: 0.25011634826660156\n",
      "Step 425, Loss: 0.25011125206947327\n",
      "Step 450, Loss: 0.25010615587234497\n",
      "Step 475, Loss: 0.2501010596752167\n",
      "Step 500, Loss: 0.25009602308273315\n",
      "Step 525, Loss: 0.25009092688560486\n",
      "Step 550, Loss: 0.25008583068847656\n",
      "Step 575, Loss: 0.25008076429367065\n",
      "Step 600, Loss: 0.25007569789886475\n",
      "Step 625, Loss: 0.25007063150405884\n",
      "Step 650, Loss: 0.25006553530693054\n",
      "Step 675, Loss: 0.250060498714447\n",
      "Step 700, Loss: 0.2500554025173187\n",
      "Step 725, Loss: 0.25005027651786804\n",
      "Step 750, Loss: 0.2500452399253845\n",
      "Step 775, Loss: 0.2500401437282562\n",
      "Step 800, Loss: 0.25003504753112793\n",
      "Step 825, Loss: 0.250029981136322\n",
      "Step 850, Loss: 0.2500249147415161\n",
      "Step 875, Loss: 0.2500198185443878\n",
      "Step 900, Loss: 0.2500147819519043\n",
      "Step 925, Loss: 0.2500097155570984\n",
      "Step 950, Loss: 0.2500046193599701\n",
      "Step 975, Loss: 0.249999538064003\n",
      "Chatbot ready\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Human:  what is python\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Categories: ['dump', 'delete-directory', 'build-automation']\n",
      "Chatbot: Updated to only delete files and to used the os.path.join() method suggested in the comments. If you also want to remove subdirectories, uncomment the elif statement.\n",
      "import os, shutil\n",
      "folder = '/path/to/folder'\n",
      "for the_file in os.listdir(folder):\n",
      "    file_path = os.path.join(folder, the_file)\n",
      "    try:\n",
      "        if os.path.isfile(file_path):\n",
      "            os.unlink(file_path)\n",
      "        #elif os.path.isdir(file_path): shutil.rmtree(file_path)\n",
      "    except Exception as e:\n",
      "        print(e)\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Human:  where can I find my windows key on the keyboard\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Categories: ['osx-leopard', 'porting', 'ora-00932']\n",
      "Chatbot: Instead of uninstalling the built-in Python, install the MacPorts version and then modify your $PATH to have the MacPorts version first.\n",
      "For example, if MacPorts installs /usr/local/bin/python, then modify your .bashrc to include PATH=/usr/local/bin:$PATH at the end.\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "Interrupted by user",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 120\u001b[0m\n\u001b[1;32m    118\u001b[0m answer \u001b[38;5;241m=\u001b[39m get_random_answer_by_category(predicted_tags)\n\u001b[1;32m    119\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mChatbot: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00manswer\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 120\u001b[0m user_input \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43minput\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mHuman: \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/ipykernel/kernelbase.py:1251\u001b[0m, in \u001b[0;36mKernel.raw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m   1249\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw_input was called, but this frontend does not support input requests.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1250\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m StdinNotImplementedError(msg)\n\u001b[0;32m-> 1251\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_input_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1252\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1253\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parent_ident\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshell\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1254\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_parent\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshell\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1255\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpassword\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1256\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/ipykernel/kernelbase.py:1295\u001b[0m, in \u001b[0;36mKernel._input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m   1292\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[1;32m   1293\u001b[0m     \u001b[38;5;66;03m# re-raise KeyboardInterrupt, to truncate traceback\u001b[39;00m\n\u001b[1;32m   1294\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInterrupted by user\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1295\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1296\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m   1297\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid Message:\u001b[39m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
     ]
    }
   ],
   "source": [
    "stemmer = PorterStemmer()\n",
    "\n",
    "def preprocess_text(text):\n",
    "    if pd.isnull(text):\n",
    "        return ''\n",
    "    text = text.lower()\n",
    "    text = ''.join([ch for ch in text if ch not in string.punctuation])\n",
    "    tokens = text.split()\n",
    "    return ' '.join([stemmer.stem(token) for token in tokens])\n",
    "\n",
    "questions_df['CleanedQuestion'] = questions_df['Body'].apply(preprocess_text)\n",
    "answers_df['CleanedAnswer'] = answers_df['Body'].apply(preprocess_text)\n",
    "\n",
    "allwords = ' '.join(questions_df['CleanedQuestion']).split()\n",
    "uniquewords = list(set(allwords))\n",
    "\n",
    "tags_list = list(set(tags_df['Tag']))\n",
    "num_classes = len(tags_list)\n",
    "\n",
    "def get_categories_for_question(question_id):\n",
    "    return tags_df[tags_df['Id'] == question_id]['Tag'].tolist()\n",
    "\n",
    "questions_df['Categories'] = questions_df['Id'].apply(get_categories_for_question)\n",
    "\n",
    "y_train = []\n",
    "for tags in questions_df['Categories']:\n",
    "    one_hot = [0] * num_classes\n",
    "    for tag in tags:\n",
    "        if tag in tags_list:\n",
    "            one_hot[tags_list.index(tag)] = 1\n",
    "    y_train.append(one_hot)\n",
    "\n",
    "def makeTextIntoNumbers(text):\n",
    "    words = text.lower().split(' ')\n",
    "    numbers = []\n",
    "    for word in words:\n",
    "        try:\n",
    "            numbers.append(uniquewords.index(word))\n",
    "        except ValueError:\n",
    "            numbers.append(0)\n",
    "    numbers = numbers + [0, 0, 0, 0, 0]\n",
    "    return numbers[:6]\n",
    "\n",
    "x_train = []\n",
    "for text in questions_df['CleanedQuestion']:\n",
    "    indices = makeTextIntoNumbers(text)\n",
    "    x_train.append(indices)\n",
    "\n",
    "x_train = torch.LongTensor(x_train)\n",
    "y_train = torch.Tensor(y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d952656-ce9c-41eb-b151-c560ec32be01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Net(\n",
      "  (embedding): Embedding(46371, 20)\n",
      "  (lstm): LSTM(20, 16, batch_first=True)\n",
      "  (fc1): Linear(in_features=16, out_features=256, bias=True)\n",
      "  (fc2): Linear(in_features=256, out_features=1154, bias=True)\n",
      ")\n",
      "Parameters [927420, 1280, 1024, 64, 64, 4096, 256, 295424, 1154]\n",
      "Step 0, Loss: 0.2504802942276001\n",
      "Step 25, Loss: 0.25042811036109924\n",
      "Step 50, Loss: 0.2503758668899536\n",
      "Step 75, Loss: 0.25032368302345276\n",
      "Step 100, Loss: 0.2502714991569519\n",
      "Step 125, Loss: 0.2502193748950958\n",
      "Step 150, Loss: 0.25016725063323975\n",
      "Step 175, Loss: 0.25011515617370605\n",
      "Step 200, Loss: 0.25006306171417236\n",
      "Step 225, Loss: 0.25001099705696106\n",
      "Step 250, Loss: 0.24995894730091095\n",
      "Step 275, Loss: 0.24990692734718323\n",
      "Step 300, Loss: 0.2498549371957779\n",
      "Step 325, Loss: 0.24980296194553375\n",
      "Step 350, Loss: 0.24975097179412842\n",
      "Step 375, Loss: 0.24969899654388428\n",
      "Step 400, Loss: 0.2496470957994461\n",
      "Step 425, Loss: 0.24959515035152435\n",
      "Step 450, Loss: 0.24954326450824738\n",
      "Step 475, Loss: 0.2494913935661316\n",
      "Step 500, Loss: 0.24943950772285461\n",
      "Step 525, Loss: 0.24938763678073883\n",
      "Step 550, Loss: 0.24933575093746185\n",
      "Step 575, Loss: 0.24928396940231323\n",
      "Step 600, Loss: 0.24923209846019745\n",
      "Step 625, Loss: 0.24918028712272644\n",
      "Step 650, Loss: 0.24912844598293304\n",
      "Step 675, Loss: 0.24907666444778442\n",
      "Step 700, Loss: 0.2490248680114746\n",
      "Step 725, Loss: 0.248973086476326\n",
      "Step 750, Loss: 0.24892129004001617\n",
      "Step 775, Loss: 0.24886953830718994\n",
      "Step 800, Loss: 0.24881777167320251\n",
      "Step 825, Loss: 0.2487660050392151\n",
      "Step 850, Loss: 0.24871425330638885\n",
      "Step 875, Loss: 0.24866250157356262\n",
      "Step 900, Loss: 0.24861076474189758\n",
      "Step 925, Loss: 0.24855901300907135\n",
      "Step 950, Loss: 0.2485072761774063\n",
      "Step 975, Loss: 0.24845553934574127\n",
      "Step 1000, Loss: 0.24840381741523743\n",
      "Step 1025, Loss: 0.2483520805835724\n",
      "Step 1050, Loss: 0.24830032885074615\n",
      "Step 1075, Loss: 0.2482486069202423\n",
      "Step 1100, Loss: 0.24819688498973846\n",
      "Step 1125, Loss: 0.24814511835575104\n",
      "Step 1150, Loss: 0.2480934113264084\n",
      "Step 1175, Loss: 0.24804167449474335\n",
      "Step 1200, Loss: 0.24798992276191711\n",
      "Step 1225, Loss: 0.24793818593025208\n",
      "Step 1250, Loss: 0.24788641929626465\n",
      "Step 1275, Loss: 0.24783466756343842\n",
      "Step 1300, Loss: 0.247782900929451\n",
      "Step 1325, Loss: 0.24773114919662476\n",
      "Step 1350, Loss: 0.24767938256263733\n",
      "Step 1375, Loss: 0.2476276010274887\n",
      "Step 1400, Loss: 0.2475758045911789\n",
      "Step 1425, Loss: 0.24752402305603027\n",
      "Step 1450, Loss: 0.24747221171855927\n",
      "Step 1475, Loss: 0.24742040038108826\n",
      "Step 1500, Loss: 0.24736855924129486\n",
      "Step 1525, Loss: 0.24731674790382385\n",
      "Step 1550, Loss: 0.24726487696170807\n",
      "Step 1575, Loss: 0.24721303582191467\n",
      "Step 1600, Loss: 0.24716117978096008\n",
      "Step 1625, Loss: 0.2471092939376831\n",
      "Step 1650, Loss: 0.24705740809440613\n",
      "Step 1675, Loss: 0.24700552225112915\n",
      "Step 1700, Loss: 0.246953547000885\n",
      "Step 1725, Loss: 0.24690163135528564\n",
      "Step 1750, Loss: 0.2468496710062027\n",
      "Step 1775, Loss: 0.24679771065711975\n",
      "Step 1800, Loss: 0.24674570560455322\n",
      "Step 1825, Loss: 0.24669374525547028\n",
      "Step 1850, Loss: 0.24664169549942017\n",
      "Step 1875, Loss: 0.24658969044685364\n",
      "Step 1900, Loss: 0.24653758108615875\n",
      "Step 1925, Loss: 0.24648554623126984\n",
      "Step 1950, Loss: 0.24643340706825256\n",
      "Step 1975, Loss: 0.24638129770755768\n",
      "Step 2000, Loss: 0.2463291883468628\n",
      "Step 2025, Loss: 0.24627703428268433\n",
      "Step 2050, Loss: 0.24622485041618347\n",
      "Step 2075, Loss: 0.24617262184619904\n",
      "Step 2100, Loss: 0.2461203783750534\n",
      "Step 2125, Loss: 0.24606813490390778\n",
      "Step 2150, Loss: 0.24601586163043976\n",
      "Step 2175, Loss: 0.24596355855464935\n",
      "Step 2200, Loss: 0.24591122567653656\n",
      "Step 2225, Loss: 0.24585887789726257\n",
      "Step 2250, Loss: 0.2458065003156662\n",
      "Step 2275, Loss: 0.24575409293174744\n",
      "Step 2300, Loss: 0.2457016408443451\n",
      "Step 2325, Loss: 0.24564920365810394\n",
      "Step 2350, Loss: 0.24559669196605682\n",
      "Step 2375, Loss: 0.2455441653728485\n",
      "Step 2400, Loss: 0.24549159407615662\n",
      "Step 2425, Loss: 0.24543902277946472\n",
      "Step 2450, Loss: 0.24538639187812805\n",
      "Step 2475, Loss: 0.24533377587795258\n",
      "Step 2500, Loss: 0.24528107047080994\n",
      "Step 2525, Loss: 0.2452283650636673\n",
      "Step 2550, Loss: 0.24517560005187988\n",
      "Step 2575, Loss: 0.24512280523777008\n",
      "Step 2600, Loss: 0.2450699806213379\n",
      "Step 2625, Loss: 0.24501711130142212\n",
      "Step 2650, Loss: 0.24496425688266754\n",
      "Step 2675, Loss: 0.2449113130569458\n",
      "Step 2700, Loss: 0.24485836923122406\n",
      "Step 2725, Loss: 0.24480536580085754\n",
      "Step 2750, Loss: 0.24475233256816864\n",
      "Step 2775, Loss: 0.24469923973083496\n",
      "Step 2800, Loss: 0.24464614689350128\n",
      "Step 2825, Loss: 0.24459296464920044\n",
      "Step 2850, Loss: 0.2445397675037384\n",
      "Step 2875, Loss: 0.24448652565479279\n",
      "Step 2900, Loss: 0.2444332391023636\n",
      "Step 2925, Loss: 0.2443799376487732\n",
      "Step 2950, Loss: 0.24432656168937683\n",
      "Step 2975, Loss: 0.24427315592765808\n",
      "Step 3000, Loss: 0.24421969056129456\n",
      "Step 3025, Loss: 0.24416618049144745\n",
      "Step 3050, Loss: 0.24411262571811676\n",
      "Step 3075, Loss: 0.24405905604362488\n",
      "Step 3100, Loss: 0.24400541186332703\n",
      "Step 3125, Loss: 0.2439517378807068\n",
      "Step 3150, Loss: 0.24389798939228058\n",
      "Step 3175, Loss: 0.24384424090385437\n",
      "Step 3200, Loss: 0.243790403008461\n",
      "Step 3225, Loss: 0.24373650550842285\n",
      "Step 3250, Loss: 0.24368257820606232\n",
      "Step 3275, Loss: 0.2436286211013794\n",
      "Step 3300, Loss: 0.24357455968856812\n",
      "Step 3325, Loss: 0.24352054297924042\n",
      "Step 3350, Loss: 0.24346637725830078\n",
      "Step 3375, Loss: 0.24341221153736115\n",
      "Step 3400, Loss: 0.24335795640945435\n",
      "Step 3425, Loss: 0.24330365657806396\n",
      "Step 3450, Loss: 0.24324931204319\n",
      "Step 3475, Loss: 0.24319493770599365\n",
      "Step 3500, Loss: 0.24314047396183014\n",
      "Step 3525, Loss: 0.24308593571186066\n",
      "Step 3550, Loss: 0.24303141236305237\n",
      "Step 3575, Loss: 0.24297676980495453\n",
      "Step 3600, Loss: 0.24292205274105072\n",
      "Step 3625, Loss: 0.2428673356771469\n",
      "Step 3650, Loss: 0.24281251430511475\n",
      "Step 3675, Loss: 0.2427576631307602\n",
      "Step 3700, Loss: 0.24270272254943848\n",
      "Step 3725, Loss: 0.24264775216579437\n",
      "Step 3750, Loss: 0.2425927221775055\n",
      "Step 3775, Loss: 0.24253763258457184\n",
      "Step 3800, Loss: 0.24248245358467102\n",
      "Step 3825, Loss: 0.24242721498012543\n",
      "Step 3850, Loss: 0.24237191677093506\n",
      "Step 3875, Loss: 0.24231652915477753\n",
      "Step 3900, Loss: 0.24226114153862\n",
      "Step 3925, Loss: 0.2422056347131729\n",
      "Step 3950, Loss: 0.24215006828308105\n",
      "Step 3975, Loss: 0.24209445714950562\n",
      "Step 4000, Loss: 0.242038756608963\n",
      "Step 4025, Loss: 0.24198301136493683\n",
      "Step 4050, Loss: 0.2419271618127823\n",
      "Step 4075, Loss: 0.24187126755714417\n",
      "Step 4100, Loss: 0.24181529879570007\n",
      "Step 4125, Loss: 0.2417592704296112\n",
      "Step 4150, Loss: 0.24170318245887756\n",
      "Step 4175, Loss: 0.24164699018001556\n",
      "Step 4200, Loss: 0.2415907233953476\n",
      "Step 4225, Loss: 0.24153439700603485\n",
      "Step 4250, Loss: 0.24147801101207733\n",
      "Step 4275, Loss: 0.24142153561115265\n",
      "Step 4300, Loss: 0.241364985704422\n",
      "Step 4325, Loss: 0.24130839109420776\n",
      "Step 4350, Loss: 0.24125166237354279\n",
      "Step 4375, Loss: 0.24119488894939423\n",
      "Step 4400, Loss: 0.2411380410194397\n",
      "Step 4425, Loss: 0.2410811334848404\n",
      "Step 4450, Loss: 0.24102409183979034\n",
      "Step 4475, Loss: 0.2409670203924179\n",
      "Step 4500, Loss: 0.2409098744392395\n",
      "Step 4525, Loss: 0.24085259437561035\n",
      "Step 4550, Loss: 0.24079529941082\n",
      "Step 4575, Loss: 0.24073787033557892\n",
      "Step 4600, Loss: 0.24068039655685425\n",
      "Step 4625, Loss: 0.24062281847000122\n",
      "Step 4650, Loss: 0.24056516587734222\n",
      "Step 4675, Loss: 0.24050742387771606\n",
      "Step 4700, Loss: 0.24044959247112274\n",
      "Step 4725, Loss: 0.24039170145988464\n",
      "Step 4750, Loss: 0.2403337061405182\n",
      "Step 4775, Loss: 0.24027562141418457\n",
      "Step 4800, Loss: 0.2402174323797226\n",
      "Step 4825, Loss: 0.24015918374061584\n",
      "Step 4850, Loss: 0.24010081589221954\n",
      "Step 4875, Loss: 0.24004243314266205\n",
      "Step 4900, Loss: 0.23998385667800903\n",
      "Step 4925, Loss: 0.23992523550987244\n",
      "Step 4950, Loss: 0.23986656963825226\n",
      "Step 4975, Loss: 0.23980778455734253\n",
      "Step 5000, Loss: 0.23974889516830444\n",
      "Step 5025, Loss: 0.239689901471138\n",
      "Step 5050, Loss: 0.239630788564682\n",
      "Step 5075, Loss: 0.23957164585590363\n",
      "Step 5100, Loss: 0.2395123988389969\n",
      "Step 5125, Loss: 0.2394530326128006\n",
      "Step 5150, Loss: 0.23939356207847595\n",
      "Step 5175, Loss: 0.23933401703834534\n",
      "Step 5200, Loss: 0.23927436769008636\n",
      "Step 5225, Loss: 0.23921462893486023\n",
      "Step 5250, Loss: 0.23915478587150574\n",
      "Step 5275, Loss: 0.2390948235988617\n",
      "Step 5300, Loss: 0.23903478682041168\n",
      "Step 5325, Loss: 0.2389746606349945\n",
      "Step 5350, Loss: 0.2389143854379654\n",
      "Step 5375, Loss: 0.2388540357351303\n",
      "Step 5400, Loss: 0.23879358172416687\n",
      "Step 5425, Loss: 0.23873303830623627\n",
      "Step 5450, Loss: 0.2386724054813385\n",
      "Step 5475, Loss: 0.2386116087436676\n",
      "Step 5500, Loss: 0.23855073750019073\n",
      "Step 5525, Loss: 0.2384897768497467\n",
      "Step 5550, Loss: 0.23842871189117432\n",
      "Step 5575, Loss: 0.23836749792099\n",
      "Step 5600, Loss: 0.2383062243461609\n",
      "Step 5625, Loss: 0.23824481666088104\n",
      "Step 5650, Loss: 0.23818330466747284\n",
      "Step 5675, Loss: 0.2381216585636139\n",
      "Step 5700, Loss: 0.23805992305278778\n",
      "Step 5725, Loss: 0.2379980981349945\n",
      "Step 5750, Loss: 0.2379361391067505\n",
      "Step 5775, Loss: 0.2378740906715393\n",
      "Step 5800, Loss: 0.2378118932247162\n",
      "Step 5825, Loss: 0.2377495914697647\n",
      "Step 5850, Loss: 0.23768717050552368\n",
      "Step 5875, Loss: 0.2376246303319931\n",
      "Step 5900, Loss: 0.23756200075149536\n",
      "Step 5925, Loss: 0.23749923706054688\n",
      "Step 5950, Loss: 0.23743635416030884\n",
      "Step 5975, Loss: 0.23737335205078125\n",
      "Step 6000, Loss: 0.2373102456331253\n",
      "Step 6025, Loss: 0.237247034907341\n",
      "Step 6050, Loss: 0.23718366026878357\n",
      "Step 6075, Loss: 0.23712021112442017\n",
      "Step 6100, Loss: 0.23705658316612244\n",
      "Step 6125, Loss: 0.23699289560317993\n",
      "Step 6150, Loss: 0.2369290441274643\n",
      "Step 6175, Loss: 0.2368650734424591\n",
      "Step 6200, Loss: 0.23680096864700317\n",
      "Step 6225, Loss: 0.23673678934574127\n",
      "Step 6250, Loss: 0.23667244613170624\n",
      "Step 6275, Loss: 0.23660796880722046\n",
      "Step 6300, Loss: 0.23654338717460632\n",
      "Step 6325, Loss: 0.23647868633270264\n",
      "Step 6350, Loss: 0.236413836479187\n",
      "Step 6375, Loss: 0.23634885251522064\n",
      "Step 6400, Loss: 0.2362837791442871\n",
      "Step 6425, Loss: 0.23621849715709686\n",
      "Step 6450, Loss: 0.23615314066410065\n",
      "Step 6475, Loss: 0.2360876500606537\n",
      "Step 6500, Loss: 0.2360220104455948\n",
      "Step 6525, Loss: 0.23595623672008514\n",
      "Step 6550, Loss: 0.23589035868644714\n",
      "Step 6575, Loss: 0.2358243316411972\n",
      "Step 6600, Loss: 0.23575812578201294\n",
      "Step 6625, Loss: 0.2356918305158615\n",
      "Step 6650, Loss: 0.23562538623809814\n",
      "Step 6675, Loss: 0.23555879294872284\n",
      "Step 6700, Loss: 0.2354920655488968\n",
      "Step 6725, Loss: 0.2354252189397812\n",
      "Step 6750, Loss: 0.23535820841789246\n",
      "Step 6775, Loss: 0.23529107868671417\n",
      "Step 6800, Loss: 0.23522377014160156\n",
      "Step 6825, Loss: 0.2351563423871994\n",
      "Step 6850, Loss: 0.2350887805223465\n",
      "Step 6875, Loss: 0.23502108454704285\n",
      "Step 6900, Loss: 0.23495320975780487\n",
      "Step 6925, Loss: 0.23488518595695496\n",
      "Step 6950, Loss: 0.2348170429468155\n",
      "Step 6975, Loss: 0.2347487509250641\n",
      "Step 7000, Loss: 0.23468030989170074\n",
      "Step 7025, Loss: 0.23461170494556427\n",
      "Step 7050, Loss: 0.23454293608665466\n",
      "Step 7075, Loss: 0.2344740480184555\n",
      "Step 7100, Loss: 0.2344050109386444\n",
      "Step 7125, Loss: 0.234335795044899\n",
      "Step 7150, Loss: 0.234266459941864\n",
      "Step 7175, Loss: 0.2341969609260559\n",
      "Step 7200, Loss: 0.23412729799747467\n",
      "Step 7225, Loss: 0.2340574711561203\n",
      "Step 7250, Loss: 0.23398752510547638\n",
      "Step 7275, Loss: 0.23391740024089813\n",
      "Step 7300, Loss: 0.23384711146354675\n",
      "Step 7325, Loss: 0.23377667367458344\n",
      "Step 7350, Loss: 0.23370607197284698\n",
      "Step 7375, Loss: 0.2336353063583374\n",
      "Step 7400, Loss: 0.23356440663337708\n",
      "Step 7425, Loss: 0.23349332809448242\n",
      "Step 7450, Loss: 0.23342205584049225\n",
      "Step 7475, Loss: 0.23335064947605133\n",
      "Step 7500, Loss: 0.23327907919883728\n",
      "Step 7525, Loss: 0.2332073599100113\n",
      "Step 7550, Loss: 0.23313546180725098\n",
      "Step 7575, Loss: 0.23306338489055634\n",
      "Step 7600, Loss: 0.23299115896224976\n",
      "Step 7625, Loss: 0.23291878402233124\n",
      "Step 7650, Loss: 0.2328462153673172\n",
      "Step 7675, Loss: 0.23277348279953003\n",
      "Step 7700, Loss: 0.23270055651664734\n",
      "Step 7725, Loss: 0.2326275110244751\n",
      "Step 7750, Loss: 0.23255427181720734\n",
      "Step 7775, Loss: 0.23248085379600525\n",
      "Step 7800, Loss: 0.23240724205970764\n",
      "Step 7825, Loss: 0.2323334962129593\n",
      "Step 7850, Loss: 0.23225952684879303\n",
      "Step 7875, Loss: 0.23218542337417603\n",
      "Step 7900, Loss: 0.2321111112833023\n",
      "Step 7925, Loss: 0.23203663527965546\n",
      "Step 7950, Loss: 0.23196199536323547\n",
      "Step 7975, Loss: 0.23188714683055878\n",
      "Step 8000, Loss: 0.23181213438510895\n",
      "Step 8025, Loss: 0.23173697292804718\n",
      "Step 8050, Loss: 0.2316615879535675\n",
      "Step 8075, Loss: 0.2315860539674759\n",
      "Step 8100, Loss: 0.23151029646396637\n",
      "Step 8125, Loss: 0.23143437504768372\n",
      "Step 8150, Loss: 0.23135827481746674\n",
      "Step 8175, Loss: 0.23128198087215424\n",
      "Step 8200, Loss: 0.23120547831058502\n",
      "Step 8225, Loss: 0.23112884163856506\n",
      "Step 8250, Loss: 0.231051966547966\n",
      "Step 8275, Loss: 0.230974942445755\n",
      "Step 8300, Loss: 0.2308976799249649\n",
      "Step 8325, Loss: 0.23082025349140167\n",
      "Step 8350, Loss: 0.23074263334274292\n",
      "Step 8375, Loss: 0.23066481947898865\n",
      "Step 8400, Loss: 0.23058682680130005\n",
      "Step 8425, Loss: 0.23050864040851593\n",
      "Step 8450, Loss: 0.2304302155971527\n",
      "Step 8475, Loss: 0.23035164177417755\n",
      "Step 8500, Loss: 0.23027285933494568\n",
      "Step 8525, Loss: 0.2301938682794571\n",
      "Step 8550, Loss: 0.2301146686077118\n",
      "Step 8575, Loss: 0.23003530502319336\n",
      "Step 8600, Loss: 0.22995570302009583\n",
      "Step 8625, Loss: 0.22987590730190277\n",
      "Step 8650, Loss: 0.2297959327697754\n",
      "Step 8675, Loss: 0.22971570491790771\n",
      "Step 8700, Loss: 0.2296353131532669\n",
      "Step 8725, Loss: 0.2295546978712082\n",
      "Step 8750, Loss: 0.22947387397289276\n",
      "Step 8775, Loss: 0.2293928563594818\n",
      "Step 8800, Loss: 0.22931158542633057\n",
      "Step 8825, Loss: 0.22923018038272858\n",
      "Step 8850, Loss: 0.2291485071182251\n",
      "Step 8875, Loss: 0.2290666401386261\n",
      "Step 8900, Loss: 0.228984534740448\n",
      "Step 8925, Loss: 0.22890228033065796\n",
      "Step 8950, Loss: 0.22881975769996643\n",
      "Step 8975, Loss: 0.2287370264530182\n",
      "Step 9000, Loss: 0.22865410149097443\n",
      "Step 9025, Loss: 0.22857092320919037\n",
      "Step 9050, Loss: 0.2284875214099884\n",
      "Step 9075, Loss: 0.2284039407968521\n",
      "Step 9100, Loss: 0.22832012176513672\n",
      "Step 9125, Loss: 0.2282360941171646\n",
      "Step 9150, Loss: 0.2281518429517746\n",
      "Step 9175, Loss: 0.22806735336780548\n",
      "Step 9200, Loss: 0.22798262536525726\n",
      "Step 9225, Loss: 0.22789770364761353\n",
      "Step 9250, Loss: 0.2278125286102295\n",
      "Step 9275, Loss: 0.22772714495658875\n",
      "Step 9300, Loss: 0.2276415079832077\n",
      "Step 9325, Loss: 0.22755569219589233\n",
      "Step 9350, Loss: 0.2274695783853531\n",
      "Step 9375, Loss: 0.2273833155632019\n",
      "Step 9400, Loss: 0.22729676961898804\n",
      "Step 9425, Loss: 0.22720998525619507\n",
      "Step 9450, Loss: 0.22712300717830658\n",
      "Step 9475, Loss: 0.2270357608795166\n",
      "Step 9500, Loss: 0.22694826126098633\n",
      "Step 9525, Loss: 0.22686052322387695\n",
      "Step 9550, Loss: 0.22677257657051086\n",
      "Step 9575, Loss: 0.22668437659740448\n",
      "Step 9600, Loss: 0.22659596800804138\n",
      "Step 9625, Loss: 0.2265072762966156\n",
      "Step 9650, Loss: 0.2264183759689331\n",
      "Step 9675, Loss: 0.22632920742034912\n",
      "Step 9700, Loss: 0.22623981535434723\n",
      "Step 9725, Loss: 0.22615015506744385\n",
      "Step 9750, Loss: 0.22606024146080017\n",
      "Step 9775, Loss: 0.2259701043367386\n",
      "Step 9800, Loss: 0.2258797287940979\n",
      "Step 9825, Loss: 0.22578905522823334\n",
      "Step 9850, Loss: 0.22569817304611206\n",
      "Step 9875, Loss: 0.2256070226430893\n",
      "Step 9900, Loss: 0.22551560401916504\n",
      "Step 9925, Loss: 0.22542396187782288\n",
      "Step 9950, Loss: 0.22533203661441803\n",
      "Step 9975, Loss: 0.22523988783359528\n",
      "Chatbot ready\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Human:  what is the purpose of photoshop\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Categories: ['buildbot', 'file', 'instance']\n",
      "Chatbot: We've had great success with TeamCity as our CI server and using nose as our test runner.  Teamcity plugin for nosetests gives you count pass/fail, readable display for failed test( that can be E-Mailed).  You can even see details of the test failures while you stack is running.  \n",
      "If of course supports things like running on multiple machines, and it's much simpler to setup and maintain than buildbot.\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Human:  is python the best language for machine learning\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Categories: ['buildbot', 'wrap', 'instance']\n",
      "Chatbot: We've had great success with TeamCity as our CI server and using nose as our test runner.  Teamcity plugin for nosetests gives you count pass/fail, readable display for failed test( that can be E-Mailed).  You can even see details of the test failures while you stack is running.  \n",
      "If of course supports things like running on multiple machines, and it's much simpler to setup and maintain than buildbot.\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Human:  how can I learn binary calculations?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Categories: ['yield', 'file', 'iis-modules']\n",
      "Chatbot: Shortcut to Grokking yield\n",
      "When you see a function with yield statements, apply this easy trick to understand what will happen:\n",
      "\n",
      "Insert a line result = [] at the start of the function.\n",
      "Replace each yield expr with result.append(expr).\n",
      "Insert a line return result at the bottom of the function.\n",
      "Yay - no more yield statements! Read and figure out code.\n",
      "Compare function to original definition.\n",
      "\n",
      "This trick may give you an idea of the logic behind the function, but what actually happens with yield is significantly different that what happens in the list based approach. In many cases the yield approach will be a lot more memory efficient and faster too. In other cases this trick will get you stuck in an infinite loop, even though the original function works just fine. Read on to learn more...\n",
      "Don't confuse your Iterables, Iterators and Generators\n",
      "First, the iterator protocol - when you write\n",
      "for x in mylist:\n",
      "    ...loop body...\n",
      "\n",
      "Python performs the following two steps:\n",
      "\n",
      "Gets an iterator for mylist:\n",
      "Call iter(mylist) -> this returns an object with a next() method (or __next__() in Python 3).\n",
      "[This is the step most people forget to tell you about]\n",
      "Uses the iterator to loop over items:\n",
      "Keep calling the next() method on the iterator returned from step 1. The return value from next() is assigned to x and the loop body is executed. If an exception StopIteration is raised from within next(), it means there are no more values in the iterator and the loop is exited.\n",
      "\n",
      "The truth is Python performs the above two steps anytime it wants to loop over the contents of an object - so it could be a for loop, but it could also be code like otherlist.extend(mylist) (where otherlist is a Python list).\n",
      "Here mylist is an iterable because it implements the iterator protocol. In a user defined class, you can implement the __iter__() method to make instances of your class iterable. This method should return an iterator. An iterator is an object with a next() method. It is possible to implement both __iter__() and next() on the same class, and have __iter__() return self. This will work for simple cases, but not when you want two iterators looping over the same object at the same time.\n",
      "So that's the iterator protocol, many objects implement this protocol:\n",
      "\n",
      "Built-in lists, dictionaries, tuples, sets, files.\n",
      "User defined classes that implement __iter__().\n",
      "Generators.\n",
      "\n",
      "Note that a for loop doesn't know what kind of object it's dealing with - it just follows the iterator protocol, and is happy to get item after item as it calls next(). Built-in lists return their items one by one, dictionaries return the keys one by one, files return the lines one by one, etc. And generators return... well that's where yield comes in:\n",
      "def f123():\n",
      "    yield 1\n",
      "    yield 2\n",
      "    yield 3\n",
      "\n",
      "for item in f123():\n",
      "    print item\n",
      "\n",
      "Instead of yield statements, if you had three return statements in f123() only the first would get executed, and the function would exit. But f123() is no ordinary function. When f123() is called, it does not return any of the values in the yield statements! It returns a generator object. Also, the function does not really exit - it goes into a suspended state. When the for loop tries to loop over the generator object, the function resumes from its suspended state at the very next line after the yield it previously returned from, executes the next line of code, in this case a yield statement, and returns that as the next item. This happens until the function exits, at which point the generator raises StopIteration, and the loop exits. \n",
      "So the generator object is sort of like an adapter - at one end it exhibits the iterator protocol, by exposing __iter__() and next() methods to keep the for loop happy. At the other end however, it runs the function just enough to get the next value out of it, and puts it back in suspended mode.\n",
      "Why Use Generators?\n",
      "Usually you can write code that doesn't use generators but implements the same logic. One option is to use the temporary list 'trick' I mentioned before. That will not work in all cases, for e.g. if you have infinite loops, or it may make inefficient use of memory when you have a really long list. The other approach is to implement a new iterable class SomethingIter that keeps state in instance members and performs the next logical step in it's next() (or __next__() in Python 3) method. Depending on the logic, the code inside the next() method may end up looking very complex and be prone to bugs. Here generators provide a clean and easy solution.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.embedding = nn.Embedding(len(uniquewords), 20)\n",
    "        self.lstm = nn.LSTM(input_size=20, hidden_size=16, num_layers=1, batch_first=True)\n",
    "        self.fc1 = nn.Linear(16, 256)\n",
    "        self.fc2 = nn.Linear(256, num_classes)\n",
    "\n",
    "    def forward(self, inp):\n",
    "        e = self.embedding(inp)\n",
    "        output, _ = self.lstm(e)\n",
    "        x = self.fc1(output[:, -1, :])\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = torch.sigmoid(x)\n",
    "        return x\n",
    "\n",
    "n = Net()\n",
    "print(\"Model\", n)\n",
    "print(\"Parameters\", [param.nelement() for param in n.parameters()])\n",
    "\n",
    "optimizer = torch.optim.SGD(n.parameters(), lr=0.01)\n",
    "loss_fn = torch.nn.MSELoss()\n",
    "\n",
    "n_steps = 10000\n",
    "for i in range(n_steps):\n",
    "    y_pred_train = n(x_train)\n",
    "    loss_train = loss_fn(y_pred_train, y_train)\n",
    "    optimizer.zero_grad()\n",
    "    loss_train.backward()\n",
    "    optimizer.step()\n",
    "    if (i % 25) == 0:\n",
    "        print(f\"Step {i}, Loss: {loss_train.item()}\")\n",
    "\n",
    "def classify(line):\n",
    "    indices = makeTextIntoNumbers(line)\n",
    "    tensor = torch.LongTensor([indices])\n",
    "    output = n(tensor).detach().numpy()\n",
    "    predicted_tags = np.argsort(output[0])[::-1]\n",
    "    return predicted_tags\n",
    "\n",
    "def clean_html(raw_html):\n",
    "    soup = BeautifulSoup(raw_html, 'html.parser')\n",
    "    text = soup.get_text()\n",
    "    text = unescape(text)\n",
    "    return text\n",
    "\n",
    "def get_random_answer_by_category(predicted_tags):\n",
    "    for aindex in predicted_tags:\n",
    "        tag = tags_list[aindex]\n",
    "        relevant_questions = questions_df[questions_df['Categories'].apply(lambda tags: tag in tags)]\n",
    "        if not relevant_questions.empty:\n",
    "            rand_index = random.choice(relevant_questions.index)\n",
    "            question_id = relevant_questions.loc[rand_index, 'Id']\n",
    "            answers = answers_df[answers_df['ParentId'] == question_id]['Body'].values\n",
    "            if len(answers) > 0:\n",
    "                answer = random.choice(answers)\n",
    "                clean_answer = clean_html(answer)\n",
    "                return clean_answer\n",
    "    return \"No matching question found.\"\n",
    "\n",
    "print(\"Chatbot ready\")\n",
    "user_input = input(\"Human: \")\n",
    "while user_input:\n",
    "    predicted_tags = classify(user_input)\n",
    "    print(f\"Predicted Categories: {[tags_list[i] for i in predicted_tags[:3]]}\")\n",
    "    answer = get_random_answer_by_category(predicted_tags)\n",
    "    print(f\"Chatbot: {answer}\")\n",
    "    user_input = input(\"Human: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f63a1d2f-33d7-4211-af5f-f678d8e43b92",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
